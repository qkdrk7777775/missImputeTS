{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e12857a3-fc92-452f-a6c5-97d9d7caf2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "imputeTS = importr('imputeTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d767370-ca3e-45d3-8b3c-c6402ab278de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n",
      "R[write to console]: URL 'https://cloud.r-project.org/src/contrib/devtools_2.4.2.tar.gz'을 시도합니다\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 371298 bytes (362 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 362 KB\n",
      "\n",
      "\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: \n",
      "R[write to console]: The downloaded source packages are in\n",
      "\t‘/tmp/Rtmp18gI0v/downloaded_packages’\n",
      "R[write to console]: \n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: 필요한 패키지를 로딩중입니다: usethis\n",
      "\n",
      "R[write to console]: Skipping install of 'missImputeTS' from a github remote, the SHA1 (5971de31) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import rpy2.robjects as ro\n",
    "ro.r('install.packages(c(\"devtools\"))')\n",
    "ro.r('library(devtools)')\n",
    "ro.r('install_github(\"qkdrk7777775/missImputeTS\")')\n",
    "missImputeTS=importr('missImputeTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09fb6601-8daf-4e73-8e8f-92fcb739fbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ducj/package/missImputeTS/missImputeTS'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0baa5ee-6f96-4281-b2be-770537db3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./data/EuStockMarkets.csv')\n",
    "df['times']=pd.to_datetime(df.times).round('1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469271ce-aecd-4d55-af84-5be07e4f1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuousTimeForm(df, time_var_name,freq):\n",
    "    df=df.set_index([time_var_name])\n",
    "    dt_range=pd.date_range(min(df.index),max(df.index),freq=freq)\n",
    "    na_date=set(dt_range)-set(df.index)\n",
    "    df_=pd.concat([df,pd.DataFrame(columns=df.columns,index=na_date)],axis=0)\n",
    "    df_=df_.sort_index()\n",
    "    df_=df_.reset_index().rename(columns={\"index\": \"times\"})\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e316879f-7d17-43ad-8c81-9353a174c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=continuousTimeForm(df,'times','1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b96fbfaf-3aae-4b8e-855f-57bd39e2b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,1:]=df.drop('times',axis=1).interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f625002-225b-4ad8-9c89-0dd9843ea685",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[1]):\n",
    "    np.random.seed(i)\n",
    "    df.iloc[np.random.choice(df.shape[0], int(df.shape[0]*.1)),i]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dae239d-3228-4969-a889-c60e563bb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_kalman(ts):\n",
    "    imputeTS = importr('imputeTS')\n",
    "    return imputeTS.na_kalman(ro.FloatVector(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98828e25-0f9f-42b0-bd2f-d949427775c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <span>FloatVector with 2613 elements.</span>\n",
       "        <table>\n",
       "        <tbody>\n",
       "          <tr>\n",
       "          \n",
       "            <td>\n",
       "            1628.750000\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            1613.630000\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            1610.070000\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            ...\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            5355.030000\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            5414.375000\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            5473.720000\n",
       "            </td>\n",
       "          \n",
       "          </tr>\n",
       "        </tbody>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "<rpy2.robjects.vectors.FloatVector object at 0x7f6326ee8600> [RTYPES.REALSXP]\n",
       "R classes: ('numeric',)\n",
       "[1628.750000, 1613.630000, 1610.070000, 1606.510000, ..., 5386.940000, 5355.030000, 5414.375000, 5473.720000]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_kalman(df.DAX.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09cd58e6-0d61-4976-8692-3ea3a37c00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r_dataframe=pandas2ri.py2rpy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88780a87-dc38-4b00-96e0-27cbce04aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmis=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b33c7528-aa81-4131-a347-dd308ce40cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmis=xmis.set_index('times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52825645-d2cb-4e4c-978d-3906959548f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MissForest Imputer for Missing Data\"\"\"\n",
    "# Author: Ashim Bhattarai\n",
    "# License: GNU General Public License v3 (GPLv3)\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted, check_array\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "__all__ = [\n",
    "    'MissForest',\n",
    "]\n",
    "\n",
    "\n",
    "class missTS(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Missing value imputation using Random Forests.\n",
    "    MissForest imputes missing values using Random Forests in an iterative\n",
    "    fashion. By default, the imputer begins imputing missing values of the\n",
    "    column (which is expected to be a variable) with the smallest number of\n",
    "    missing values -- let's call this the candidate column.\n",
    "    The first step involves filling any missing values of the remaining,\n",
    "    non-candidate, columns with an initial guess, which is the column mean for\n",
    "    columns representing numerical variables and the column mode for columns\n",
    "    representing categorical variables. After that, the imputer fits a random\n",
    "    forest model with the candidate column as the outcome variable and the\n",
    "    remaining columns as the predictors over all rows where the candidate\n",
    "    column values are not missing.\n",
    "    After the fit, the missing rows of the candidate column are\n",
    "    imputed using the prediction from the fitted Random Forest. The\n",
    "    rows of the non-candidate columns act as the input data for the fitted\n",
    "    model.\n",
    "    Following this, the imputer moves on to the next candidate column with the\n",
    "    second smallest number of missing values from among the non-candidate\n",
    "    columns in the first round. The process repeats itself for each column\n",
    "    with a missing value, possibly over multiple iterations or epochs for\n",
    "    each column, until the stopping criterion is met.\n",
    "    The stopping criterion is governed by the \"difference\" between the imputed\n",
    "    arrays over successive iterations. For numerical variables (num_vars_),\n",
    "    the difference is defined as follows:\n",
    "     sum((X_new[:, num_vars_] - X_old[:, num_vars_]) ** 2) /\n",
    "     sum((X_new[:, num_vars_]) ** 2)\n",
    "    For categorical variables(cat_vars_), the difference is defined as follows:\n",
    "    sum(X_new[:, cat_vars_] != X_old[:, cat_vars_])) / n_cat_missing\n",
    "    where X_new is the newly imputed array, X_old is the array imputed in the\n",
    "    previous round, n_cat_missing is the total number of categorical\n",
    "    values that are missing, and the sum() is performed both across rows\n",
    "    and columns. Following [1], the stopping criterion is considered to have\n",
    "    been met when difference between X_new and X_old increases for the first\n",
    "    time for both types of variables (if available).\n",
    "    Parameters\n",
    "    ----------\n",
    "    NOTE: Most parameter definitions below are taken verbatim from the\n",
    "    Scikit-Learn documentation at [2] and [3].\n",
    "    max_iter : int, optional (default = 10)\n",
    "        The maximum iterations of the imputation process. Each column with a\n",
    "        missing value is imputed exactly once in a given iteration.\n",
    "    decreasing : boolean, optional (default = False)\n",
    "        If set to True, columns are sorted according to decreasing number of\n",
    "        missing values. In other words, imputation will move from imputing\n",
    "        columns with the largest number of missing values to columns with\n",
    "        fewest number of missing values.\n",
    "    missing_values : np.nan, integer, optional (default = np.nan)\n",
    "        The placeholder for the missing values. All occurrences of\n",
    "        `missing_values` will be imputed.\n",
    "    copy : boolean, optional (default = True)\n",
    "        If True, a copy of X will be created. If False, imputation will\n",
    "        be done in-place whenever possible.\n",
    "    criterion : tuple, optional (default = ('mse', 'gini'))\n",
    "        The function to measure the quality of a split.The first element of\n",
    "        the tuple is for the Random Forest Regressor (for imputing numerical\n",
    "        variables) while the second element is for the Random Forest\n",
    "        Classifier (for imputing categorical variables).\n",
    "    n_estimators : integer, optional (default=100)\n",
    "        The number of trees in the forest.\n",
    "    max_depth : integer or None, optional (default=None)\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples.\n",
    "    min_samples_split : int, float, optional (default=2)\n",
    "        The minimum number of samples required to split an internal node:\n",
    "        - If int, then consider `min_samples_split` as the minimum number.\n",
    "        - If float, then `min_samples_split` is a fraction and\n",
    "          `ceil(min_samples_split * n_samples)` are the minimum\n",
    "          number of samples for each split.\n",
    "    min_samples_leaf : int, float, optional (default=1)\n",
    "        The minimum number of samples required to be at a leaf node.\n",
    "        A split point at any depth will only be considered if it leaves at\n",
    "        least ``min_samples_leaf`` training samples in each of the left and\n",
    "        right branches.  This may have the effect of smoothing the model,\n",
    "        especially in regression.\n",
    "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "        - If float, then `min_samples_leaf` is a fraction and\n",
    "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "          number of samples for each node.\n",
    "    min_weight_fraction_leaf : float, optional (default=0.)\n",
    "        The minimum weighted fraction of the sum total of weights (of all\n",
    "        the input samples) required to be at a leaf node. Samples have\n",
    "        equal weight when sample_weight is not provided.\n",
    "    max_features : int, float, string or None, optional (default=\"auto\")\n",
    "        The number of features to consider when looking for the best split:\n",
    "        - If int, then consider `max_features` features at each split.\n",
    "        - If float, then `max_features` is a fraction and\n",
    "          `int(max_features * n_features)` features are considered at each\n",
    "          split.\n",
    "        - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None, then `max_features=n_features`.\n",
    "        Note: the search for a split does not stop until at least one\n",
    "        valid partition of the node samples is found, even if it requires to\n",
    "        effectively inspect more than ``max_features`` features.\n",
    "    max_leaf_nodes : int or None, optional (default=None)\n",
    "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
    "        Best nodes are defined as relative reduction in impurity.\n",
    "        If None then unlimited number of leaf nodes.\n",
    "    min_impurity_decrease : float, optional (default=0.)\n",
    "        A node will be split if this split induces a decrease of the impurity\n",
    "        greater than or equal to this value.\n",
    "        The weighted impurity decrease equation is the following::\n",
    "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "        if ``sample_weight`` is passed.\n",
    "    bootstrap : boolean, optional (default=True)\n",
    "        Whether bootstrap samples are used when building trees.\n",
    "    oob_score : bool (default=False)\n",
    "        Whether to use out-of-bag samples to estimate\n",
    "        the generalization accuracy.\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    verbose : int, optional (default=0)\n",
    "        Controls the verbosity when fitting and predicting.\n",
    "    warm_start : bool, optional (default=False)\n",
    "        When set to ``True``, reuse the solution of the previous call to fit\n",
    "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
    "        new forest. See :term:`the Glossary <warm_start>`.\n",
    "    class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or \\\n",
    "    None, optional (default=None)\n",
    "        Weights associated with classes in the form ``{class_label: weight}``.\n",
    "        If not given, all classes are supposed to have weight one. For\n",
    "        multi-output problems, a list of dicts can be provided in the same\n",
    "        order as the columns of y.\n",
    "        Note that for multioutput (including multilabel) weights should be\n",
    "        defined for each class of every column in its own dict. For example,\n",
    "        for four-class multilabel classification weights should be\n",
    "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
    "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
    "        The \"balanced\" mode uses the values of y to automatically adjust\n",
    "        weights inversely proportional to class frequencies in the input data\n",
    "        as ``n_samples / (n_classes * np.bincount(y))``\n",
    "        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
    "        weights are computed based on the bootstrap sample for every tree\n",
    "        grown.\n",
    "        For multi-output, the weights of each column of y will be multiplied.\n",
    "        Note that these weights will be multiplied with sample_weight (passed\n",
    "        through the fit method) if sample_weight is specified.\n",
    "        NOTE: This parameter is only applicable for Random Forest Classifier\n",
    "        objects (i.e., for categorical variables).\n",
    "    Attributes\n",
    "    ----------\n",
    "    statistics_ : Dictionary of length two\n",
    "        The first element is an array with the mean of each numerical feature\n",
    "        being imputed while the second element is an array of modes of\n",
    "        categorical features being imputed (if available, otherwise it\n",
    "        will be None).\n",
    "    References\n",
    "    ----------\n",
    "    * [1] Stekhoven, Daniel J., and Peter Bühlmann. \"MissForest—non-parametric\n",
    "      missing value imputation for mixed-type data.\" Bioinformatics 28.1\n",
    "      (2011): 112-118.\n",
    "    * [2] https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.\n",
    "      RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\n",
    "    * [3] https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.\n",
    "      RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "    * [4] https://github.com/epsilon-machine/missingpy/blob/master/missingpy/missforest.py\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from missingpy import MissForest\n",
    "    >>> nan = float(\"NaN\")\n",
    "    >>> X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n",
    "    >>> imputer = MissForest(random_state=1337)\n",
    "    >>> imputer.fit_transform(X)\n",
    "    Iteration: 0\n",
    "    Iteration: 1\n",
    "    Iteration: 2\n",
    "    array([[1.  , 2. , 3.92 ],\n",
    "           [3.  , 4. , 3. ],\n",
    "           [2.71, 6. , 5. ],\n",
    "           [8.  , 8. , 7. ]])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_iter=10, decreasing=False, missing_values=np.nan,\n",
    "                 copy=True, n_estimators=100, criterion=('mse', 'gini'),\n",
    "                 max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                 min_weight_fraction_leaf=0.0, max_features='auto',\n",
    "                 max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 bootstrap=True, oob_score=False, n_jobs=-1, random_state=None,\n",
    "                 verbose=0, warm_start=False, class_weight=None):\n",
    "\n",
    "        self.max_iter = max_iter\n",
    "        self.decreasing = decreasing\n",
    "        self.missing_values = missing_values\n",
    "        self.copy = copy\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.bootstrap = bootstrap\n",
    "        self.oob_score = oob_score\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.warm_start = warm_start\n",
    "        self.class_weight = class_weight\n",
    "\n",
    "    def _get_mask(self,X, value_to_mask):\n",
    "        \"\"\"Compute the boolean mask X == missing_values.\"\"\"\n",
    "        if value_to_mask == \"NaN\" or np.isnan(value_to_mask):\n",
    "            return np.isnan(X)\n",
    "        else:\n",
    "            return X == value_to_mask\n",
    "\n",
    "    def _miss_forest(self, Ximp, mask):\n",
    "        \"\"\"The missForest algorithm\"\"\"\n",
    "\n",
    "        # Count missing per column\n",
    "        col_missing_count = mask.sum(axis=0)\n",
    "\n",
    "        # Get col and row indices for missing\n",
    "        missing_rows, missing_cols = np.where(mask)\n",
    "\n",
    "        if self.num_vars_ is not None:\n",
    "            # Only keep indices for numerical vars\n",
    "            \n",
    "            for num_var in self.num_vars_:\n",
    "                Ximp[:,num_var]=np.array(na_kalman(Ximp[:,num_var]))\n",
    "#             keep_idx_num = np.in1d(missing_cols, self.num_vars_)\n",
    "#             missing_num_rows = missing_rows[keep_idx_num]\n",
    "#             missing_num_cols = missing_cols[keep_idx_num]\n",
    "\n",
    "#             # Make initial guess for missing values\n",
    "#             col_means = np.full(Ximp.shape[1], fill_value=np.nan)\n",
    "#             col_means[self.num_vars_] = self.statistics_.get('col_means')\n",
    "#             Ximp[missing_num_rows, missing_num_cols] = np.take(\n",
    "#                 col_means, missing_num_cols)\n",
    "\n",
    "#             # Reg criterion\n",
    "            reg_criterion = self.criterion if type(self.criterion) == str \\\n",
    "                else self.criterion[0]\n",
    "\n",
    "            # Instantiate regression model\n",
    "            rf_regressor = RandomForestRegressor(\n",
    "                n_estimators=self.n_estimators,\n",
    "                criterion=reg_criterion,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "                max_features=self.max_features,\n",
    "                max_leaf_nodes=self.max_leaf_nodes,\n",
    "                min_impurity_decrease=self.min_impurity_decrease,\n",
    "                bootstrap=self.bootstrap,\n",
    "                oob_score=self.oob_score,\n",
    "                n_jobs=self.n_jobs,\n",
    "                random_state=self.random_state,\n",
    "                verbose=self.verbose,\n",
    "                warm_start=self.warm_start)\n",
    "\n",
    "        # If needed, repeat for categorical variables\n",
    "        if self.cat_vars_ is not None:\n",
    "            # Calculate total number of missing categorical values (used later)\n",
    "            n_catmissing = np.sum(mask[:, self.cat_vars_])\n",
    "\n",
    "            # Only keep indices for categorical vars\n",
    "            keep_idx_cat = np.in1d(missing_cols, self.cat_vars_)\n",
    "            missing_cat_rows = missing_rows[keep_idx_cat]\n",
    "            missing_cat_cols = missing_cols[keep_idx_cat]\n",
    "\n",
    "            # Make initial guess for missing values\n",
    "            col_modes = np.full(Ximp.shape[1], fill_value=np.nan)\n",
    "            col_modes[self.cat_vars_] = self.statistics_.get('col_modes')\n",
    "            Ximp[missing_cat_rows, missing_cat_cols] = np.take(col_modes, missing_cat_cols)\n",
    "\n",
    "            # Classfication criterion\n",
    "            clf_criterion = self.criterion if type(self.criterion) == str \\\n",
    "                else self.criterion[1]\n",
    "\n",
    "            # Instantiate classification model\n",
    "            rf_classifier = RandomForestClassifier(\n",
    "                n_estimators=self.n_estimators,\n",
    "                criterion=clf_criterion,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "                max_features=self.max_features,\n",
    "                max_leaf_nodes=self.max_leaf_nodes,\n",
    "                min_impurity_decrease=self.min_impurity_decrease,\n",
    "                bootstrap=self.bootstrap,\n",
    "                oob_score=self.oob_score,\n",
    "                n_jobs=self.n_jobs,\n",
    "                random_state=self.random_state,\n",
    "                verbose=self.verbose,\n",
    "                warm_start=self.warm_start,\n",
    "                class_weight=self.class_weight)\n",
    "\n",
    "        # 2. misscount_idx: sorted indices of cols in X based on missing count\n",
    "        misscount_idx = np.argsort(col_missing_count)\n",
    "        # Reverse order if decreasing is set to True\n",
    "        if self.decreasing is True:\n",
    "            misscount_idx = misscount_idx[::-1]\n",
    "\n",
    "        # 3. While new_gammas < old_gammas & self.iter_count_ < max_iter loop:\n",
    "        self.iter_count_ = 0\n",
    "        gamma_new = 0\n",
    "        gamma_old = np.inf\n",
    "        gamma_newcat = 0\n",
    "        gamma_oldcat = np.inf\n",
    "        col_index = np.arange(Ximp.shape[1])\n",
    "\n",
    "        while (\n",
    "                gamma_new < gamma_old or gamma_newcat < gamma_oldcat) and \\\n",
    "                self.iter_count_ < self.max_iter:\n",
    "\n",
    "            # 4. store previously imputed matrix\n",
    "            Ximp_old = np.copy(Ximp)\n",
    "            if self.iter_count_ != 0:\n",
    "                gamma_old = gamma_new\n",
    "                gamma_oldcat = gamma_newcat\n",
    "            # 5. loop\n",
    "            for s in misscount_idx:\n",
    "                # Column indices other than the one being imputed\n",
    "                s_prime = np.delete(col_index, s)\n",
    "\n",
    "                # Get indices of rows where 's' is observed and missing\n",
    "                obs_rows = np.where(~mask[:, s])[0]\n",
    "                mis_rows = np.where(mask[:, s])[0]\n",
    "\n",
    "                # If no missing, then skip\n",
    "                if len(mis_rows) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Get observed values of 's'\n",
    "                yobs = Ximp[obs_rows, s]\n",
    "\n",
    "                # Get 'X' for both observed and missing 's' column\n",
    "                xobs = Ximp[np.ix_(obs_rows, s_prime)]\n",
    "                xmis = Ximp[np.ix_(mis_rows, s_prime)]\n",
    "\n",
    "                # 6. Fit a random forest over observed and predict the missing\n",
    "                if self.cat_vars_ is not None and s in self.cat_vars_:\n",
    "                    rf_classifier.fit(X=xobs, y=yobs)\n",
    "                    # 7. predict ymis(s) using xmis(x)\n",
    "                    ymis = rf_classifier.predict(xmis)\n",
    "                    # 8. update imputed matrix using predicted matrix ymis(s)\n",
    "                    Ximp[mis_rows, s] = ymis\n",
    "                else:\n",
    "                    rf_regressor.fit(X=xobs, y=yobs)\n",
    "                    # 7. predict ymis(s) using xmis(x)\n",
    "                    ymis = rf_regressor.predict(xmis)\n",
    "                    # 8. update imputed matrix using predicted matrix ymis(s)\n",
    "                    Ximp[mis_rows, s] = ymis\n",
    "\n",
    "            # 9. Update gamma (stopping criterion)\n",
    "            if self.cat_vars_ is not None:\n",
    "                gamma_newcat = np.sum(\n",
    "                    (Ximp[:, self.cat_vars_] != Ximp_old[:, self.cat_vars_])) / n_catmissing\n",
    "            if self.num_vars_ is not None:\n",
    "                gamma_new = np.sum((Ximp[:, self.num_vars_] - Ximp_old[:, self.num_vars_]) ** 2) / np.sum((Ximp[:, self.num_vars_]) ** 2)\n",
    "\n",
    "            print(\"Iteration:\", self.iter_count_)\n",
    "            self.iter_count_ += 1\n",
    "\n",
    "        return Ximp_old\n",
    "\n",
    "    def fit(self, X, y=None, cat_vars=None):\n",
    "        \"\"\"Fit the imputer on X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            Input data, where ``n_samples`` is the number of samples and\n",
    "            ``n_features`` is the number of features.\n",
    "        cat_vars : int or array of ints, optional (default = None)\n",
    "            An int or an array containing column indices of categorical\n",
    "            variable(s)/feature(s) present in the dataset X.\n",
    "            ``None`` if there are no categorical variables in the dataset.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check data integrity and calling arguments\n",
    "        force_all_finite = False if self.missing_values in [\"NaN\",np.nan] else True\n",
    "\n",
    "        X = check_array(X, accept_sparse=False, dtype=np.float64,force_all_finite=force_all_finite, copy=self.copy)\n",
    "\n",
    "        # Check for +/- inf\n",
    "        if np.any(np.isinf(X)):\n",
    "            raise ValueError(\"+/- inf values are not supported.\")\n",
    "\n",
    "        # Check if any column has all missing\n",
    "        mask = self._get_mask(X, self.missing_values)\n",
    "        if np.any(mask.sum(axis=0) >= (X.shape[0])):\n",
    "            raise ValueError(\"One or more columns have all rows missing.\")\n",
    "\n",
    "        # Check cat_vars type and convert if necessary\n",
    "        if cat_vars is not None:\n",
    "            if type(cat_vars) == int:\n",
    "                cat_vars = [cat_vars]\n",
    "            elif type(cat_vars) == list or type(cat_vars) == np.ndarray:\n",
    "                if np.array(cat_vars).dtype != int:\n",
    "                    raise ValueError(\n",
    "                        \"cat_vars needs to be either an int or an array \"\n",
    "                        \"of ints.\")\n",
    "            else:\n",
    "                raise ValueError(\"cat_vars needs to be either an int or an array \"\n",
    "                                 \"of ints.\")\n",
    "\n",
    "        # Identify numerical variables\n",
    "        num_vars = np.setdiff1d(np.arange(X.shape[1]), cat_vars)\n",
    "        num_vars = num_vars if len(num_vars) > 0 else None\n",
    "\n",
    "        # First replace missing values with NaN if it is something else\n",
    "        if self.missing_values not in ['NaN', np.nan]:\n",
    "            X[np.where(X == self.missing_values)] = np.nan\n",
    "\n",
    "        # Now, make initial guess for missing values\n",
    "        col_means = np.nanmean(X[:, num_vars], axis=0) if num_vars is not None else None\n",
    "        col_modes = mode(\n",
    "            X[:, cat_vars], axis=0, nan_policy='omit')[0] if cat_vars is not \\\n",
    "                                                           None else None\n",
    "\n",
    "        self.cat_vars_ = cat_vars\n",
    "        self.num_vars_ = num_vars\n",
    "        self.statistics_ = {\"col_means\": col_means, \"col_modes\": col_modes}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Impute all missing values in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            The input data to complete.\n",
    "        Returns\n",
    "        -------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            The imputed dataset.\n",
    "        \"\"\"\n",
    "        # Confirm whether fit() has been called\n",
    "        check_is_fitted(self, [\"cat_vars_\", \"num_vars_\", \"statistics_\"])\n",
    "\n",
    "        # Check data integrity\n",
    "        force_all_finite = False if self.missing_values in [\"NaN\",np.nan] else True\n",
    "        X = check_array(X, accept_sparse=False, dtype=np.float64,force_all_finite=force_all_finite, copy=self.copy)\n",
    "\n",
    "        # Check for +/- inf\n",
    "        if np.any(np.isinf(X)):\n",
    "            raise ValueError(\"+/- inf values are not supported.\")\n",
    "\n",
    "        # Check if any column has all missing\n",
    "        mask = self._get_mask(X, self.missing_values)\n",
    "        if np.any(mask.sum(axis=0) >= (X.shape[0])):\n",
    "            raise ValueError(\"One or more columns have all rows missing.\")\n",
    "\n",
    "        # Get fitted X col count and ensure correct dimension\n",
    "        n_cols_fit_X = (0 if self.num_vars_ is None else len(self.num_vars_)) \\\n",
    "            + (0 if self.cat_vars_ is None else len(self.cat_vars_))\n",
    "        _, n_cols_X = X.shape\n",
    "\n",
    "        if n_cols_X != n_cols_fit_X:\n",
    "            raise ValueError(\"Incompatible dimension between the fitted \"\n",
    "                             \"dataset and the one to be transformed.\")\n",
    "\n",
    "        # Check if anything is actually missing and if not return original X                             \n",
    "        mask = self._get_mask(X, self.missing_values)\n",
    "        if not mask.sum() > 0:\n",
    "            warnings.warn(\"No missing value located; returning original \"\n",
    "                          \"dataset.\")\n",
    "            return X\n",
    "\n",
    "        # row_total_missing = mask.sum(axis=1)\n",
    "        # if not np.any(row_total_missing):\n",
    "        #     return X\n",
    "\n",
    "        # Call missForest function to impute missing\n",
    "        X = self._miss_forest(X, mask)\n",
    "\n",
    "        # Return imputed dataset\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        \"\"\"Fit MissForest and impute all missing values in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            Input data, where ``n_samples`` is the number of samples and\n",
    "            ``n_features`` is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            Returns imputed dataset.\n",
    "        \"\"\"\n",
    "        return self.fit(X, **fit_params).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4301cc58-25bb-4c45-985c-911a02baf01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = missTS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aaf7f39-24a5-4a99-bf60-f47402dd8347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1628.75 , 1678.1  , 1772.8  , 2443.6  ],\n",
       "       [1613.63 , 1688.5  , 1750.5  , 2460.2  ],\n",
       "       [1610.07 , 1683.55 , 1734.25 , 2454.2  ],\n",
       "       ...,\n",
       "       [5355.03 , 7552.6  , 3951.7  , 5399.5  ],\n",
       "       [5414.375, 7614.45 , 3973.35 , 5427.25 ],\n",
       "       [5473.72 , 7676.3  , 4016.983, 5864.069]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.fit_transform(xmis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
